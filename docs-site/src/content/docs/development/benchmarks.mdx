---
title: Performance Benchmarks
description: Performance metrics and hardware recommendations
---

# Performance Benchmarks

Performance benchmarks for Mailpilot across different configurations and hardware setups.

## Test Methodology

All benchmarks were conducted using:
- **Test emails**: 1000 emails with varying complexity
- **Classification**: Standard prompts with folder organization
- **Database**: SQLite with WAL mode
- **Measurement**: Average of 10 runs with outliers removed

## Hardware Specifications

### Test Systems

**Baseline** (Minimum Requirements):
- **CPU**: Intel Core i5-1135G7 (4 cores @ 2.4GHz)
- **RAM**: 8 GB DDR4
- **Storage**: SATA SSD (500 MB/s)
- **Network**: 100 Mbps

**Recommended** (Optimal Performance):
- **CPU**: AMD Ryzen 7 5800X (8 cores @ 3.8GHz)
- **RAM**: 16 GB DDR4
- **Storage**: NVMe SSD (3500 MB/s)
- **Network**: 1 Gbps

**High-Performance** (Local Models):
- **CPU**: AMD Ryzen 9 5950X (16 cores @ 3.4GHz)
- **RAM**: 32 GB DDR4
- **GPU**: NVIDIA RTX 4090 (24 GB VRAM)
- **Storage**: NVMe SSD (7000 MB/s)
- **Network**: 1 Gbps

## LLM Provider Comparison

### Latency Benchmarks

Average time to classify a single email (includes network + API processing):

| Provider | Model | Avg Latency | P95 Latency | P99 Latency |
|----------|-------|-------------|-------------|-------------|
| **OpenAI** | gpt-4o-mini | 450ms | 780ms | 1200ms |
| **OpenAI** | gpt-4o | 850ms | 1400ms | 2100ms |
| **Anthropic** | claude-3-haiku | 520ms | 890ms | 1300ms |
| **Anthropic** | claude-3.5-sonnet | 980ms | 1600ms | 2400ms |
| **Ollama** | llama3.2:3b | 180ms | 290ms | 450ms |
| **Ollama** | llama3.1:8b | 420ms | 680ms | 950ms |
| **Ollama** | qwen2.5:14b | 890ms | 1400ms | 2100ms |

*Latency measured on Recommended hardware. Local models (Ollama) assume GPU acceleration.*

### Throughput Benchmarks

Emails processed per hour (concurrent processing, 100 emails/account):

| Provider | Model | Baseline | Recommended | High-Performance |
|----------|-------|----------|-------------|------------------|
| **OpenAI** | gpt-4o-mini | 6,800 | 8,000 | 8,000* |
| **Anthropic** | claude-3-haiku | 5,900 | 6,900 | 6,900* |
| **Ollama** | llama3.2:3b | 12,000 | 20,000 | 35,000 |
| **Ollama** | llama3.1:8b | 5,400 | 8,600 | 18,000 |

*API-based models are bottlenecked by rate limits, not local hardware*

## Cost Analysis

### API Provider Costs

Cost to classify 10,000 emails (typical monthly volume for heavy users):

| Provider | Model | Cost per 1K emails | 10K emails | 100K emails |
|----------|-------|-------------------|------------|-------------|
| **OpenAI** | gpt-4o-mini | $0.003 | $0.03 | $0.30 |
| **OpenAI** | gpt-4o | $0.015 | $0.15 | $1.50 |
| **Anthropic** | claude-3-haiku | $0.0025 | $0.025 | $0.25 |
| **Anthropic** | claude-3.5-sonnet | $0.030 | $0.30 | $3.00 |

*Assumes average 200 tokens per classification (150 input + 50 output)*

### Local Model Costs

One-time hardware investment for local processing:

| Configuration | Hardware Cost | Monthly Electricity | Break-even (vs API) |
|---------------|---------------|---------------------|---------------------|
| **CPU-only** | $800 | ~$5 | N/A (too slow) |
| **Entry GPU** | $1,500 | ~$15 | ~25K emails/month |
| **High-end GPU** | $3,500 | ~$30 | ~50K emails/month |

*Break-even calculated against gpt-4o-mini pricing*

## Resource Usage

### Memory Footprint

| Component | Idle | 1 Account | 5 Accounts | 10 Accounts |
|-----------|------|-----------|------------|-------------|
| **Backend** | 45 MB | 80 MB | 150 MB | 280 MB |
| **Dashboard** | - | 25 MB | 25 MB | 25 MB |
| **Database** | 2 MB | 15 MB | 60 MB | 120 MB |
| **Ollama** (llama3.2:3b) | 2.1 GB | 2.3 GB | 2.5 GB | 2.8 GB |
| **Ollama** (qwen2.5:14b) | 8.2 GB | 8.5 GB | 8.9 GB | 9.3 GB |

*Ollama memory includes model loaded in VRAM/RAM*

### CPU Utilization

Average CPU usage during email processing:

| Scenario | Baseline | Recommended | High-Performance |
|----------|----------|-------------|------------------|
| **Idle (IMAP IDLE)** | 1% | &lt;1% | &lt;1% |
| **Processing (API)** | 8-15% | 5-10% | 3-7% |
| **Processing (Ollama 3b)** | 45-80% | 30-60% | 15-35% |
| **Processing (Ollama 14b)** | 95-100%* | 75-90% | 40-65% |

*100% CPU on Baseline hardware = significant slowdown*

### Network Bandwidth

| Provider Type | Idle | Light (100/day) | Heavy (1000/day) |
|---------------|------|-----------------|------------------|
| **API Providers** | &lt;1 KB/s | 5-10 KB/s | 50-80 KB/s |
| **Local Models** | &lt;1 KB/s | &lt;1 KB/s | &lt;1 KB/s |

*Network usage is minimal - mainly IMAP heartbeat and API calls*

## Scalability Benchmarks

### Email Volume

How Mailpilot handles different volumes (using gpt-4o-mini):

| Daily Volume | Accounts | Avg Latency | Database Size (30d) | RAM Usage |
|--------------|----------|-------------|---------------------|-----------|
| 100 | 1-2 | 450ms | 50 MB | 100 MB |
| 500 | 3-5 | 480ms | 200 MB | 180 MB |
| 1,000 | 5-10 | 520ms | 400 MB | 300 MB |
| 5,000 | 10-20 | 650ms | 2 GB | 600 MB |
| 10,000 | 20-50 | 800ms | 4 GB | 1.2 GB |

*Latency increases with scale due to database size and concurrent processing*

### Account Limits

Practical limits based on testing:

| Hardware | Max Accounts | Max Emails/Day | Notes |
|----------|-------------|----------------|-------|
| **Baseline** | 10 | 1,000 | Slow with local models |
| **Recommended** | 25 | 5,000 | Comfortable for most users |
| **High-Performance** | 100+ | 20,000+ | Limited by IMAP connections |

## Optimization Strategies

### For High Volume

**1. Use Faster Models**
```yaml
llm_providers:
  - name: fast
    provider: openai
    model: gpt-4o-mini  # 3x faster than gpt-4o
```

**2. Reduce Context**
```yaml
attachments:
  max_extracted_chars: 5000  # Limit attachment text
```

**3. Batch Processing**
```yaml
concurrency: 5  # Process 5 emails concurrently
```

### For Cost Reduction

**1. Use Local Models**
```yaml
llm_providers:
  - name: local
    provider: ollama
    model: llama3.2:3b  # $0 per email
```

**2. Selective Processing**
```yaml
accounts:
  - name: important
    llm_provider: claude-3.5-sonnet  # Expensive, accurate
  - name: newsletters
    llm_provider: ollama-3b  # Free, good enough
```

### For Latency Reduction

**1. Local Models with GPU**
- Use NVIDIA GPU with CUDA support
- Choose smaller models (3b-8b parameters)
- Enable GPU acceleration in Ollama

**2. IMAP IDLE**
- Use providers that support IDLE (instant notifications)
- Reduces polling overhead
- Lower latency for real-time processing

**3. Geographic Proximity**
- Choose API provider region closest to you
- Self-host Ollama on local network
- Reduces network latency

## Real-World Performance

### Personal Inbox (500 emails/day)

**Setup:**
- Provider: OpenAI gpt-4o-mini
- Accounts: 2 (Gmail, Outlook)
- Hardware: Recommended

**Results:**
- **Processing time**: &lt;30 seconds behind email arrival
- **Monthly cost**: ~$0.05
- **CPU usage**: 5-8% average
- **RAM usage**: 150 MB

### Business Inbox (2000 emails/day)

**Setup:**
- Provider: Claude 3 Haiku (primary), Ollama llama3.2:3b (newsletters)
- Accounts: 10
- Hardware: High-Performance

**Results:**
- **Processing time**: &lt;15 seconds behind email arrival
- **Monthly cost**: ~$1.50 (mostly Claude API)
- **CPU usage**: 12-20% average
- **RAM usage**: 600 MB (including Ollama model)

## Bottleneck Analysis

### Common Bottlenecks

1. **IMAP Connection** (No IDLE support)
   - Impact: 30-60s delay per email
   - Solution: Use providers with IDLE support

2. **LLM API Rate Limits**
   - Impact: Emails queued during bursts
   - Solution: Increase `concurrency` or use local models

3. **Database Locks** (High concurrency)
   - Impact: Occasional retry delays
   - Solution: Already mitigated with WAL mode

4. **Slow Models** (Local inference)
   - Impact: Processing lag on large volumes
   - Solution: Use smaller models or GPU acceleration

## Next Steps

- [Optimize your configuration →](/docs/configuration/global)
- [Choose the right LLM provider →](/docs/llm-providers/)
- [Monitor performance in dashboard →](/docs/features/dashboard)
